<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[@beam2d]]></title>
  <link href="http://beam2d.github.io/atom.xml" rel="self"/>
  <link href="http://beam2d.github.io/"/>
  <updated>2015-04-06T22:47:11+09:00</updated>
  <id>http://beam2d.github.io/</id>
  <author>
    <name><![CDATA[Seiya Tokui]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OR学会誌に寄稿しました]]></title>
    <link href="http://beam2d.github.io/blog/2015/04/06/orsj/"/>
    <updated>2015-04-06T21:25:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2015/04/06/orsj</id>
    <content type="html"><![CDATA[<p>オペレーションズ・リサーチ学会機関誌の 2015 年 4 月号に 7 ページの記事を書きました．</p>

<ul>
  <li><a href="http://www.orsj.or.jp/e-library/elcorsj.html#6004">オペレーションズ・リサーチ　 4月号　2015年　Vol.60　No.4</a></li>
  <li><a href="http://www.orsj.or.jp/archive2/or60-4/or60_4_191.pdf">(PDF) 最適化から見たディープラーニングの考え方</a></li>
</ul>

<!-- more -->

<p>上の記事は「ニューロサイエンスと数理モデリング」という特集のうち，深層学習を紹介する 3 つの記事の一つとして書きました．
OR 学会誌のバックナンバーを読みながら，最適化の観点で深層学習の話を整理したいと思い上のような記事を書きました．
なので，どちらかというと自分の頭を整理するのが主目的みたいになっています．
入門・概要的な内容もお願いされていたので，導入は機械学習自体の説明から入っています（限りなく必要最低限という感じにしてしまいましたが）．</p>

<p>この原稿を書いたのは昨年末です．
深層学習界隈はスピードが速く，それから進んだ部分もありますので，いくつか記事の内容を補足します．</p>

<h3 id="section">活性化関数</h3>

<p>ReLU $\max(0, x)$ がよく使われますが，最近は超パラメータ $0&lt;a&lt;1$ を使った leaky ReLU $\max(ax, x)$ も使われます．
<a href="http://arxiv.org/abs/1502.01852">[He et al.]</a> ではさらにこの $a$ も学習する Parametric ReLU が提案されています．
これらは最終的な性能に貢献します．</p>

<h3 id="section-1">最適化手法</h3>

<p>モーメンタム法や RMSprop がよく使われますが，最新の研究では Adam <a href="http://arxiv.org/abs/1412.6980">[Kingma&amp;Ba]</a> が使われ始めています．
Adam は，モーメンタム法と RMSprop を組み合わせたような手法です．
超パラメータがいくつかありますが，試したところでは初期学習率を除いて論文にあるデフォルトの値でうまくいくことが多いようです．
凸な目的関数についてのみですが，理論解析（リグレット上界）もあります．</p>

<h3 id="section-2">解の分布の理論解析</h3>

<p>記事では，ガウス確率場に従う非凸関数の停留点分布を解析した結果を紹介しましたが，この話は実際にはニューラルネットの目的関数については何も言っていません．
そこで，ReLU を用いたニューラルネットによる二値分類について <a href="http://arxiv.org/abs/1412.0233">[Choromanska et al.]</a> で実際に解析されました．
これは，記事で紹介した研究と同様に統計力学の結果を用いています．
Choromanska らの論文では，最適化を（少々むりやりな仮定をいくつか入れて）スピングラスモデルのエネルギー最小化に帰着し，統計力学の方で解析されていたエネルギー停留点の分布に関する結果をニューラルネットの言葉に翻訳しています．
結果として，指数の小さな停留点に遭遇する確率はエネルギーがある程度大きい（損失が大きい）ところではほとんどゼロで，それより小さなエネルギーバンドに入ると段々大きくなっていくということがわかりました．
このエネルギーバンドはさらに指数の値ごとに帯状に分かれていて，閾値をまたぐごとに指数の大きい停留点が徐々になくなっていきます．
指数の大きい停留点は勾配法で抜け出しやすいため，この結果は比較的せまいエネルギーバンドまでは勾配法で到達できること，そのエネルギーバンドに突入すると途端に最適化が難しくなることを示唆しています．</p>

<h3 id="section-3">モデル圧縮</h3>

<p>原稿の流れ上，モデル圧縮の話は浅いニューラルネットの学習という文脈だけで紹介しました．
モデル圧縮については，蒸留 (distillation) と呼ばれるテクニックが <a href="http://arxiv.org/abs/1503.02531">[Hinton et al.]</a> で提案され，注目されています．
これはモデル圧縮の中でも，アンサンブル結果を一つのニューラルネットに集約するという文脈を意識した内容になっています．</p>

<p>同じテクニックを使う話として FitNets <a href="http://arxiv.org/abs/1412.6550">[Romero et al.]</a> という論文もあります．
FitNets の目的は，狭くて深いネットワークの最適化です．
ニューラルネットは，狭くて深いほど最適化が難しいと言われています．
FitNets では，広くて（比較的）浅いネットワークを教師として，より狭くて深いネットワークを最適化します．
結果として，もとのネットワークより少ないパラメータ数でより識別性能の良いネットワークが得られます．
パラメータ数が減る上に精度も上がるということで，モデル圧縮の可能性を感じる結果です．</p>

<h3 id="section-4">最適化のテクニック</h3>

<p>ほかに原稿を書いたあとに発表された重要なテクニックとして，バッチ正規化 <a href="http://arxiv.org/abs/1502.03167">[Ioffe &amp; Szegedy]</a> に触れないわけにはいきません．
これは，各活性化関数の手前でミニバッチを正規化するというものです．
これまでも，入力データを白色化することで学習が大幅に加速することは知られていました．
また，各層で値のスケールが一定になるようにパラメータ初期値の分散をうまく選ぶことで，特に初期の最適化が加速することも知られていました．
バッチ正規化は，このスケール合わせと白色化をすべての層で明示的に行うことで，どんなに深いネットワークでも勾配が減衰せずに逆伝搬するようにします．
論文の報告では，ILSVRC のような大規模な学習課題で，（同じエラー率に到達するまでの反復回数という意味で）14 倍程度の高速化が達成され，state of the art なエラー率を達成しています．</p>

<hr />

<p>改めてまとめてみると，この 3 ヶ月程度でこんなに進んだんだなあと驚きます．
深層学習は最適化に絞ってもまだまだトピックが山ほど出てきそうです．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NIPS2014読み会で深層半教師あり学習の論文を紹介しました]]></title>
    <link href="http://beam2d.github.io/blog/2015/01/24/ssl-deep/"/>
    <updated>2015-01-24T15:30:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2015/01/24/ssl-deep</id>
    <content type="html"><![CDATA[<p>今週の火曜日 (1/20) に東大で <a href="http://connpass.com/event/10568/">NIPS2014 読み会</a> が開かれました．
NIPS 自体の参加者数が増えているのと同様に，読み会も去年にくらべてさらに多くの人が集まりました．
その中で僕もひとつ論文を紹介しました．</p>

<!-- more -->

<iframe src="http://beam2d.github.io//www.slideshare.net/slideshow/embed_code/43686520" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""> </iframe>
<div style="margin-bottom:5px"> <strong> <a href="http://beam2d.github.io//www.slideshare.net/beam2d/semisupervised-learning-with-deep-generative-models" title="論文紹介 Semi-supervised Learning with Deep Generative Models" target="_blank">論文紹介 Semi-supervised Learning with Deep Generative Models</a> </strong> from <strong><a href="http://beam2d.github.io//www.slideshare.net/beam2d" target="_blank">Seiya Tokui</a></strong> </div>

<p>紹介した論文の著者は，スライド中にも出てくる変分 AutoEncoder の考案者です．
変分 AE では，生成モデルと認識モデルをそれぞれニューラルネットで定義して，確率変数としてそれらの出力をパラメータとする正規分布を使いました．
生成モデルを認識モデルで近似したときの変分下界は，認識モデルに関する期待値の形をしています．</p>

<p>このように，最適化の対象となる分布に関する期待値の最適化は，一般には REINFORCE アルゴリズムによる勾配法を使います．
REINFORCE アルゴリズムは，期待値を積分で書いた時に，積の微分を使って勾配を計算し，それを対数微分の公式 $x’ = (\log x)’ x$ をつかって $q(\mathbf z | \mathbf x)$ に関する期待値の形に戻すことで導出できます．
これをサンプリングにより近似しますが，この方法で得られる勾配のサンプルは大きな分散を持ち，学習が非常に遅いという問題があります．
変分 AE の学習アルゴリズム（スライド中で SGVB および SBP と書いたもの）は，正規乱数を用いた場合にこの分散の問題を解決した手法です．</p>

<p>今回紹介した論文は，この変分 AE の半教師あり学習への応用です．
かなり素直な拡張で，査読でも straight forward ではないかと指摘されていました．
個人的には実験が面白いのと，生成・認識モデルはもっと知られても良いと思ったので，紹介させていただきました．</p>

<p>さて，この生成モデルと認識モデルを同時に学習するという話は，僕の知る限りでは <a href="https://www.cs.toronto.edu/~hinton/helmholtz.html">1994～1996 年あたりに Hinton がやっていた仕事</a>が源流のようです．
当時 Hinton は，各変数が二値を取るような多層のベイジアンネット（Sigmoid Belief Net などとも呼ばれます）をおもな対象としていました．
生成モデル $p$ と認識モデル $q$ の間で，$KL(q \| p)$ を最小化するような学習法を Helmholz Machine と呼んでいます．
Helmholz Machine の学習方法として 1995 年に Hinton が考案したのが Wake-Sleep アルゴリズムです．
このアルゴリズムに対しては，最近 <a href="http://arxiv.org/abs/1406.2751">Reweighted Wake-Sleep</a> という発展形も提案されています．
変分 AE では，Sigmoid Belief Net にかわって，潜在変数と観測変数の関係を決定的な多層パーセプトロンで与えています．</p>

<p>深層学習の文脈で生成モデルというと Deep Belief Net や Deep Boltzmann Machine が主流です．
これらは非常に高い表現力を持っていて，おもしろい実験結果がたくさんあります．
一方，これらのモデルは計算において扱いづらいのが問題です．
たとえば，Deep Belief Net は，単体では推論できません．
Deep Boltzmann Machine は推論が可能ですが，MCMC を必要とし，計算コストがかかります．</p>

<p>変分 AE のような生成・認識モデルは，推論が簡単なのがウリです．
生成も認識もただの伝承サンプリングですみ，MCMC が必要ありません．
MCMC が必要ない深層生成モデルとしては，ほかに NADE など，Sigmoid Belief Net を拡張する研究があります．</p>

<p>もうひとつ，伝承サンプリングが可能な新しいモデルとして <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets">Generative Adversarial Nets</a> というのもあります．
このモデルは，ニューラルネットで定義した生成モデルと，やはりニューラルネットで定義した識別モデルからなります．
識別モデルは，与えられたデータが真のデータ分布（または経験分布）から生成されたものなのか，あるいは学習中の生成モデルから生成されたものなのかを判別します．
学習のときにはこれらを同時に最適化します．
GAN は，学習のときに推論が必要ないというおもしろい特徴をもっています．
一方で，裏を返せば GAN は推論器を学習しないため，推論をしたかったら別に推論モデルを学習させる必要があります．</p>

<p>変分 AE や GAN では，深さを実現しているのは決定的なニューラルネット（多層パーセプトロン）です．
これを誤差逆伝搬による勾配法で学習します．
よって，これらの手法において，ニューラルネット部分の設計や最適化には，教師あり学習で培われてきた技術（dropout など）がほぼそのまま転用できます．
このことは，データの規模が大きくなってきたときに大きなアドバンテージになるでしょう．</p>

<p>さて，そういうわけで，伝承サンプリングできる深層生成モデルの学習は少しずつ盛り上がっていて，個人的に注目している分野です．
まだ小規模なデータでの実験にとどまっていますが，そろそろ大規模データでの検証もされていくのではないかと期待しています．</p>

<p>ちなみに，この論文の著者は <a href="http://arxiv.org/abs/1412.6980">Adam</a> という最適化手法を ICLR2015 に提出しています．
これはモーメンタム法を RMSprop に組み込んだような手法です．
今回読み会で紹介した論文でも「モーメンタム法を使った RMSprop」を使っていましたが，それをちゃんとまとめたものが Adam ではないかと思います．
凸な場合のリグレット解析もされていて，実験結果も安定して良いようで，こちらも気になります．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2014年ふりかえり]]></title>
    <link href="http://beam2d.github.io/blog/2014/12/31/my-2014/"/>
    <updated>2014-12-31T10:50:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2014/12/31/my-2014</id>
    <content type="html"><![CDATA[<p>2014年を個人的にふりかえります。</p>

<!-- more -->

<h2 id="deep-learning">Deep Learning</h2>

<p>今年は、Deep Learning を追い続ける一年でした。
画像認識の Deep Learning は、テクニックとしてはだいぶ成熟してきた感じがします。
今年は特に検出の性能が大幅に上がりました（GoogLeNet + MultiBox など）。
自然言語処理の Deep Learning もスピード感が出てきました。
それ以外にも、RNN がまともに学習できるようになってきて、面白いタスクが続々出てきました。</p>

<p>教師なし学習は相変わらず、着々と進みつつもまだブレイクスルーはない印象です。
深い有向確率モデルの研究が今年は盛んでした。
一方、RBM ベースの手法は成熟してきて、モデルをいじって応用する段階にあるようです。</p>

<p>理論的にはあまり進展がありませんでした。
Dropout の解析や応用は進んでいる印象があります。
今はどちらかというと、実験を通じて現象論的に何が起きているかを説明するアプローチのほうが、得られるものが多いようです。</p>

<p>今年は Google の論文に遭遇する回数がとても増えた気がします。</p>

<h2 id="section">対外発表</h2>

<p>今年はいろいろなところで Deep Learning の話をさせていただきました。</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/30334293 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen=""></iframe>

<p><a href="http://connpass.com/event/4728/">NIPS 読み会</a>（1月23日、<a href="http://research.preferred.jp/2014/01/nips/">まとめ</a>）では、DeViSE（およびConSE）というマルチモーダル埋め込みの手法を紹介しました。
これらは ConvNet の出力をラベル単語の word2vec 表現に合わせるというものです。</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/30649933 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen=""></iframe>

<p><a href="http://www.sig-agi.org/wba/2">全脳アーキテクチャ勉強会</a>（1月30日、<a href="http://research.preferred.jp/2014/01/whole-brain-architecture/">僕のまとめ</a>）では、Deep Learning の様々な手法を展覧会的に紹介しました。
ちょっと五月雨式にやりすぎた感があります。
DeepMind の買収に触れてますが、まだ1年経っていないことがおどろきです。</p>

<p>また、上の NIPS 読み会と合わせて、ワイドサイズのスライドに挑戦しました。
スライド自体はスペースが広く感じられ、とても作りやすかったですが、その後スライドを 4:3 のフォーマットに移植しづらくて苦労しました。
そういうわけで、4:3 のプロジェクターが一般的なうちは、要請されない限り 4:3 で作ったほうが良いと思いました。</p>

<p><a href="http://bdm.change-jp.com/?p=1010">原財団セミナー</a>（2月4日）では、機械学習の概要を解説しました。</p>

<p><a href="http://www.ipsj.or.jp/event/seminar/2014/program/2014-1.html">情報処理学会セミナー</a>（6月9日）では、教師あり学習の問題設定から Deep Learning の基礎までを話しました。
予測がテーマだったので、レコメンデーションへの応用に触れました。</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/38391798 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen=""></iframe>

<p><a href="http://www.jnns.org/conference/DeepLearning.html">神経回路学会セミナー</a>（8月26日）では、Deep Learning を実装・利用面から解説しました。
4 月に社内で <a href="https://github.com/BVLC/caffe">Caffe</a> のソースコード解説をしたことがあり、それを汎用化して話しました。
実装の話だけでは使えないので、利用するときの注意点もまとめました。</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/40900689 " width="595" height="446" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen=""></iframe>

<p>PFIセミナー（10月30日、<a href="http://www.ustream.tv/recorded/54670646">録画あり</a>）では、Recurrent Net をまとめました。
メインは Long Short-Term Memory (LSTM) の解説で、後半にいくつか刺激的な応用・発展形を挙げています。
LSTM RNN はまだうまくいき始めたところで、画像認識でいうと 2011〜2012 年くらいの地点にいるのかなと思います。</p>

<h2 id="section-1">仕事</h2>

<p>ブログに書けないことも多いですが、今年は仕事面でもいろいろ変化がありました。
特に、所属が PFI から PFN に移りました。</p>

<p>今年も夏季インターンを行い、僕は松元さんのメンターを務めました（<a href="http://www.ustream.tv/recorded/53153399">インターン最終発表の録画</a>）。
Deep Q Network の拡張がテーマで、面白い議論をたくさんさせていただきました。</p>

<p>また、今年は会社の先輩方と共同でオンライン学習の本を書いていました。
IBIS チュートリアルでこっそり宣伝があったようです。
本はまだ完成していませんが、来年出る予定です。
ちなみに、執筆に関しては今思うと反省点が山のようにあります。
今後に生きると良いのですが。</p>

<p>US への出張は 4 月に一回ありました。</p>

<h2 id="section-2">旅行</h2>

<p>今年は旅行に 3 回も行きました（福岡、サイパン、函館）。</p>

<p>サイパンは、新婚旅行以来はじめての海外旅行でした。
海がきれいでとても気持ちよかったです。
雨季だったためにほとんど曇っていたのが心残りですが、最終日は晴れていて、ギリギリまで海にいました。</p>

<p>函館旅行が初めての北海道でした。
とても寒かった。
函館山からの夜景がきれいでした。
2 泊 3 日でいろいろ回るのにはちょうど良い場所でした。</p>

<h2 id="section-3">消費</h2>

<p>今年はあまり大きな買い物をしませんでした（多分）。</p>

<p>コンスタントに週 8 本前後のアニメを見続けました。
最近はそこそこおもしろいアニメが多いと感じます。
それとは別に『ちはやふる』のアニメにハマり、原作漫画も一気読みしました。</p>

<p>冬コミに行きました。
<a href="http://n-linear.org/">学部時代に僕がいたサークル</a>がついに壁をゲットしたと聞き、それを見に行くのが主な目的でした。
本当に壁にいて、とても感動しました。
ノンリニアも立派になったものだなあ。
ちなみに、コミケに行くのは 5 年ぶりです。
僕が初めてコミケに行ってから今年で干支が一周しましたが、あそこはずっと同じような雰囲気で良いですね。
いろんなサークルのブースに囲まれていると、創作意欲をかきたてられます（最近なにもしてないですが）。</p>

<hr />

<p>以上、全体的に珍しくアクティブな一年でした。
来年のことは、来年書こうと思います。
それではよいお年を！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deep Learning と Underfitting]]></title>
    <link href="http://beam2d.github.io/blog/2014/11/09/deep-learning-and-underfitting/"/>
    <updated>2014-11-09T09:34:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2014/11/09/deep-learning-and-underfitting</id>
    <content type="html"><![CDATA[<p>深層学習 (Deep Learning) で使うニューラルネットは複雑なモデルなので正則化するのが普通ですが、深層学習が成功した要因としては Overfitting（過学習）の回避よりも Underfitting の回避のほうが重要な気がしています。
いくつか深層学習の具体的な技術を取り上げて Underfitting との関わりを考えてみたのでまとめます。</p>

<!-- more -->

<p>TODO</p>

<p>書く内容</p>

<ul>
  <li>学習の目標hあ汎化誤差の小さいモデル・パラメータを得ること</li>
  <li>誤差には汎化誤差、訓練誤差、最適化誤差がある</li>
  <li>訓練誤差と比べて汎化誤差が大きい状態が overfitting</li>
  <li>訓練誤差が大きい状態が underfitting</li>
  <li>Underfitting の原因は二種類：モデルの表現力が足りなくてデータの本質的な傾向を捉えられない、または最適化に失敗している</li>
  <li>最適化を無視すれば、汎化誤差と訓練誤差はトレードオフになり、モデルの表現力を調節することになる（モデル選択または正則化によって）</li>
  <li>NN では最適化誤差が無視できない</li>
  <li>NN については局所解を得るだけでも良さそうということがわかってきている（Backprop 論文や Saddle Free Newton 論文など）</li>
  <li>では深層学習の具体的な進歩例を見ていくと最適化の観点から何が見えてくるか？</li>
</ul>

<h2 id="cnn">CNN</h2>

<ul>
  <li>結合重みに制約を入れて良い解の存在を保ったままパラメータの自由度を小さくする</li>
  <li>かつ小さくなった空間には制約からくる構造由来のプラトーがない</li>
  <li>Pooling もパラメータを少なくする</li>
</ul>

<h2 id="dnn">DNN</h2>

<ul>
  <li>層ごとに異なる抽象度を表すようにパラメータの構造を導入することで各パラメータが直接関わる問題を単純化する、と思われている</li>
  <li>浅いNNだとパラメータは入力データの表現に依存した局所性の中でしか動けない</li>
  <li>深いNNの深い層のパラメータは異なる距離尺度における局所性の中で動ける</li>
  <li>こうすることで深い層を通じて（入力データの表現から見た時に）大きく異る予測器へも短いパスで変化できるため、最適化がしやすくなる</li>
</ul>

<h2 id="pretraining">Pretraining</h2>

<ul>
  <li>Sigmoid DNNだと入力に近い層で勾配が消える問題がある</li>
  <li>これはつまり、深い層と浅い層でパラメータの勾配のスケールが違うために、浅い層のパラメータ次元の方向には目的関数がプラトーに近くなっていることを表す</li>
  <li>Pretraining時には深さ由来のプラトーが現れないため、学習の初期におけるプラトーの問題をある程度回避できる</li>
  <li>ただし教師あり学習の目的関数を反映した方向に進むとは限らないため、効果は限定的</li>
</ul>

<h2 id="rectifier">Rectifier</h2>

<ul>
  <li>勾配消失の問題が起きなくなる、つまり深い層と浅い層でパラメータの勾配のスケールが近づく</li>
  <li>代わりにパラメータのスケールが大きすぎるとすぐに勾配が爆発するため、正則化等によりパラメータのスケールを小さく抑える必要がある</li>
</ul>

<h2 id="section">途中の層からの教師信号</h2>

<ul>
  <li>Very Deep NNでRectifierを使う場合、パラメータのスケールを小さく抑えるせいで浅い層のパラメータの勾配が再び小さくなってしまう</li>
  <li>GoogLeNet論文などではこの問題に対して途中の層から教師信号を流すことで勾配を大きく保つ</li>
  <li>効果としては pretraining と似たものだと考えられるが、識別的に学習するのもポイントかもしれない</li>
  <li></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[半教師あり学習はドメイン適応か？]]></title>
    <link href="http://beam2d.github.io/blog/2014/09/30/is-ssl-domain-adaptation/"/>
    <updated>2014-09-30T22:03:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2014/09/30/is-ssl-domain-adaptation</id>
    <content type="html"><![CDATA[<p>データがある分布に従う、とはどういうことなんでしょうか。</p>

<!-- more -->

<p>半教師あり学習は、教師ありデータが少ない時に、いっぱいある教師なしデータを使って汎化性能を上げようという問題です。
一方、ドメイン適応は、識別したいデータと教師データが異なる分布に従うときに、識別したいデータにおける識別性能を上げようという問題です。</p>

<p>半教師あり学習の設定で、生成モデルを考えます。
データの分布を適当なモデル $p(x|\theta)$ で推定します。
ここで $\theta$ はパラメータです。
事前分布を $p(\theta)$ とします。
このとき、教師ありデータセット $S$ を観測するとパラメータの事後分布 $p(\theta|S)$ が決まります。
つまりモデル $\{p(x|\theta)\}_\theta$ の中で $S$ を生成しそうな分布の確率分布が得られます。
一方、教師なしデータも含めたデータセット $U$ を観測して得られる事後分布 $p(\theta|U)$ は一般に異なる分布です。
つまり $U$ を生成しそうな分布の確率分布は $S$ のそれとは異なります。</p>

<p>普通、半教師あり学習では i.i.d. なサンプル集合 $S$ と $U$ が同じ分布に従うことを仮定します。
その仮定を外すと、上記の通り $S$ と $U$ は異なる分布に従うと推定されるように思えます。
というよりむしろ、これらが異なる分布になるから半教師あり学習をする意味があるんではないかという気さえしてきます。
$U$ の方がデータが多いわけなので、$p(\theta|U)$ は $p(\theta|S)$ より低分散な分布になると予想されます。
そこから推定される $p(x)$ は教師ありデータと教師なしデータで異なるはずです。
例えばベイズっぽく推定するなら $X\in\{S, U\}$ として $p(x|X)=\mathbb E_{\theta|X}p(x|\theta)$ となり、$X=S$ の場合と $X=U$ の場合で異なる分布が得られる（かもしれない）わけです。</p>

<p>すると教師ありデータ $S$ と教師なしデータ $U$ で推定される分布が違うわけですが、最終的に汎化して欲しいのはデータをたくさん含む $U$ の方なわけです。
それで冒頭の定義を見てみると、これはドメイン適応問題として見ることもできるのだろうか、と思いました。
教師ありデータと教師なしデータが異なる分布に従うと思っても、そんなに変ではないのではないか、と。
結局は問題のモデル化をどっちにするかという話ですし、$S$ と $U$ が同じ分布に従うという仮定はある種の事前知識なわけで、それが正しい事前知識ならば使った方が良い結果が得られるようにも思うし、一般にドメイン適応の方が難しい問題だと思うので、そこに帰着させるのが意味のある議論なのかよくわからないのですが。
直感としては、$S$ がとても小さくてスパースに分布しているとき、サンプリングがたまたま偏る確率は低くないわけで、そうすると $S$ と $U$ は異なるパラメータでモデル化した方がうまくいくようなケースというのがあるのではないかという気がしています。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Softplus関数]]></title>
    <link href="http://beam2d.github.io/blog/2014/03/02/softplus/"/>
    <updated>2014-03-02T22:38:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2014/03/02/softplus</id>
    <content type="html"><![CDATA[<p>今日、Caffeというナウいニューラルネット実装を読んでいたら次の行で？？？となりました。</p>

<p><a href="https://github.com/BVLC/caffe/blob/v0.9/src/caffe/layers/bnll_layer.cu#L20">https://github.com/BVLC/caffe/blob/v0.9/src/caffe/layers/bnll_layer.cu#L20</a></p>

<p>数式で書くと（logは自然対数）</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

f(x)=\begin{cases}
  x+\log(1+e^{-x}) & \text{if}\,x > 0, \\
  \log(1+e^x) & \text{otherwise}.
\end{cases}
 %]]&gt;</script>

<p>もっとオシャレに書くと</p>

<script type="math/tex; mode=display">f(x)=\max(0, x)+\log(1+e^{-\vert x\vert}).</script>

<p>これが数値計算的なテクニックなのか、近似も含んでるのか、僕が知らないactivation関数なのかパッと見判断つかなかったのですが、微分してみたら両方シグモイド関数になりました（で、よく見たらすぐ下に導関数の実装も書いてあった）。
まず滑らかなのかこれ、というところでちょっと驚く。</p>

<p>さて、シグモイドを導関数に持つのは、softplusと呼ばれる関数です（これも忘れててしばらく考えてた）。</p>

<script type="math/tex; mode=display">\text{softplus}(x)=\log(1+e^x).</script>

<p>これはRectified linear Unit (ReLU) とかpositive partと呼ばれる式 $\max(0, x)$ と似た性質を持っていて、かつ滑らかで勾配が（厳密には）消えない関数としてニューラルネット界隈で時々使われます。
つまり上の式はsoftplus関数なのでした。</p>

<p>実際、softplus関数にlogsumexpでやるような大きなexpの追い出しをやると $x&gt;0$ における最初の式が導出できます。</p>

<script type="math/tex; mode=display">
\log(1+e^x)=\log(e^x(e^{-x}+1))=x+\log(1+e^{-x}).
</script>

<p>当たり前ですが頑張ればもとの（オシャレな方の）式からsoftplusを導出することもできます。positive partとnegative partが出てきてちょっと面白い（個人の感想です）。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
\max(0, x)+\log(1+e^{-\vert x\vert})
&=\log(e^{\max(0, x)}(1+e^{-\vert x\vert}))\\
&=\log(e^{\max(0, x)}+e^{\max(0, x)-\vert x\vert})\\
&=\log(e^{\max(0, x)}+e^{\min(0, x)})\\
&=\log(1+e^x).
\end{align*}
 %]]&gt;</script>

<p>気づいてしまえばそうなんですが、最初softplus関数のことが頭になかったので、しばらく考えてしまいました。
数値計算テクむずい。</p>

<p>さて、するとこの式はReLUとsoftplus関数の差を表す式ということになります。</p>

<script type="math/tex; mode=display">\text{softplus}(x)-\max(0, x)=\log(1+e^{-\vert x\vert}).</script>

<p>右辺の関数 $\log(1+e^{-\vert x\vert})$ をプロットしてみると次のようになります。</p>

<p><img src="http://beam2d.github.io/images/softplus-relu.png" alt="Softplus(x) - ReLU(x)" /></p>

<p>式から明らかですが、僕のぱっと見の想像と違ってSoftplusとReLUの差は左右対称な形をしていました。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2014年]]></title>
    <link href="http://beam2d.github.io/blog/2014/01/05/happy-new-year/"/>
    <updated>2014-01-05T09:00:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2014/01/05/happy-new-year</id>
    <content type="html"><![CDATA[<p>三が日は終わってしまったけれど、あけましておめでとうございます。</p>

<!-- more -->

<p>去年の個人的に印象的なイベントを公私合わせて思い出してみました。</p>

<ul>
  <li>1月: 妻の実家訪問 兼 温泉旅行</li>
  <li>3月: 新婚旅行でフランスに行って大雪に遭う</li>
  <li>6月: 会社の人たちと箱根旅行でケーブルカーに恐怖する</li>
  <li>6月: 初めて国際会議(ICML, SIGMOD)に行く（聞きに行っただけ）</li>
  <li>9月: 福岡旅行で梅ケ枝餅がおいしかった</li>
  <li>10月: はじめてのEXPO</li>
  <li>10月: サンフランシスコ出張</li>
</ul>

<p>去年は遠くに行く機会がとても増えたように思います。
国際会議楽しかったけど、論文通したことないのに行ったのはずるっぽかった。</p>

<p>生活面ではカフェインをあまりとらなくなりました。
6月に急性カフェイン中毒になったのと、8月に急に心拍数が上がる現象が何度か起きて、それ以来カフェインをできるだけ取らない生活にシフトしました。
病院で検査してもらいましたが特に異常なく、それは安心しましたが、カフェインとミント類はしばらく控えるよう言われました<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。
今はときどきコーヒー一杯ぐらい飲めるようになりましたが、基本的にはノンカフェイン、飲んでも一日にコーヒーか紅茶を一杯までという生活を続けています。
カフェイン取らなくても生きていけるということを知りました。
ただ、日本はノンカフェイン生活にやさしくない国だということも知りました。
ほとんどの喫茶店にはデカフェ置いてないですし（例外はスタバとタリーズくらい？）、ランチのドリンクがコーヒー紅茶しかないようなお店もあります。
コーヒーは好きなので、今でもスタバとタリーズでデカフェの豆買ってきて家では主にそれを飲んでいます（ときどき普通のコーヒー）。
アメリカでの少ない経験上、どの喫茶店・ホテルに行ってもデカフェが置いてあったので、そこは羨ましいなあと思います。</p>

<p>その頃から体調管理を気をつけるようになって、特に睡眠の勉強をしました。
寝る3時間前から食べないとか、2時間前にお風呂はいるとかそういうやつです。
朝も奥さんが家を出る8時前にできるだけ合わせるようになりました。
これで生活リズムはだいぶ改善されて、この半年くらいは風邪を引かずに済んでいます。
これは個人的にはすごいことで、こんなに長い間風邪を引かないのは久しぶりです。
眠い日も減りました。</p>

<p>去年は旅行とか外食とかそういうイベント事で後手後手になりがちだったのがよくなかったと反省していて、今年の標語は「先手先手で動く」にしました。
ちなみに新年早々、初詣どうするかですでに後手後手になってもう失敗してるんですが、今日からの標語ということにしました。
私生活の細かい目標は以下のとおり。</p>

<ul>
  <li>筋肉・体力をつける、手の冷え性を治す</li>
  <li>5回旅行行く、うち1回は海外に行く</li>
  <li>50冊本読む（漫画ラノベ除く）、記録をつける</li>
  <li>20回ブログを書く</li>
  <li>1つ何か作って公開する</li>
</ul>

<p>今年も一年よろしくお願いいたします。</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>ミントも心拍数を上げるそうです。これは知らなかった。<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Denoising Autoencoderとその一般化]]></title>
    <link href="http://beam2d.github.io/blog/2013/12/23/dae-and-its-generalization/"/>
    <updated>2013-12-23T22:48:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2013/12/23/dae-and-its-generalization</id>
    <content type="html"><![CDATA[<p><a href="http://qiita.com/advent-calendar/2013/machinelearning">Machine Learning Advenc Calendar 2013</a>の23日目担当の得居です。
<a href="https://preferred.jp">株式会社Preferred Infrastructure</a>で<a href="http://jubat.us">Jubatus</a>を作ったりしています。</p>

<p>今日は深層学習(deep learning)の話です。
深層学習はこの2年ほどで専門外の人にも知れ渡るほどに大流行しました。
データさえ大量にあればテクニック次第で他の手法を圧倒する性能を達成できることから、特に大量のデータを持つ大企業において大々的な参入が相次ぎました。</p>

<p>主に流行っているのは教師あり学習です。
補助として教師なし学習による事前学習(pretraining)も、特に音声認識のタスクにおいては行われているようですが、画像認識を中心に事前学習なしでもテクニック次第で学習できるという見方が強まっています。</p>

<p>一方で教師なしデータからの学習はブレイクスルー待ちといった雰囲気です。
Deep Belief NetworksやDeep Boltzmann Machinesなど「無名隠れ変数 (anonymous latent variables)」を用いた生成モデルが先行して詳しく解析されていますが、その学習は教師あり学習のようにはうまくいっていないのが現状です<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。</p>

<p>今日お話するDenoising Autoencoder (DAE)はそんな教師なし深層学習で用いられる手法・モデルの一つです。
RBM系の深層生成モデルとは別の系統になりますが、DAEをベースにした確率的な深層モデルの理論が数年遅れて最近発展してきているので、今日はその概要をまとめて書きたいと思います。
主にPascal VincentとYoshua Bengioによるこの5年間の研究の流れに沿って、以下の様な順に書きます。</p>

<ul>
  <li>Autoencoderの定式化</li>
  <li>DAEの定式化とそのオリジナルの解釈</li>
  <li>DAEのスコアマッチングとしての解釈</li>
  <li>DAEの確率モデルとしての一般化</li>
  <li>一般化DAEのためのWalkbackアルゴリズム</li>
  <li>隠れ変数を入れたさらに一般のモデルとその特徴</li>
</ul>

<p>分量の割に時間がかかっていないので、説明が変だったりまどろっこしい部分があるかもしれません。
気づいた点がございましたらぜひ<a href="https://twitter.com/beam2d">@beam2d</a>までお知らせください。</p>

<!-- more -->

<h2 id="autoencoder-ae">Autoencoder (AE)</h2>

<p>まずAEの定式化を紹介します。
入力層 $x\in\mathbb R^d$ に対して隠れ層 $y=f_\theta(x)=s(Wx+b)\in\mathbb R^{d_\text{h}}$ と復元層 $x^r=g_{\theta’}(y)=s_r(W’y+c)\in\mathbb R^d$ を定義します。
ただし $\theta=(W, b)$, $\theta’=(W’, c)$ は共に学習すべきパラメータです。
$s, s_r$ は活性化関数(activation function)と呼ばれます。
$s_r$ は入力がバイナリ値ならシグモイド関数、一般の実数値なら恒等関数にします。
$W’=W^\top$ という制約(tied weights)を置くと性能が良いことが知られています（解釈の一つが後ほど出てきます）。
関数 $f_\theta$ はエンコーダ、$g_{\theta’}$ はデコーダと呼ばれます。</p>

<p>こうして定義された復元層 $x^r$ が入力層 $x$ にできるだけ近くなるようにエンコーダとデコーダのパラメータ $\theta, \theta’$ を学習します。
訓練データ $\mathcal D={x_1, \dots, x_n}$ に対して損失関数 $L(x, x^r)$ の平均値を最小化します:</p>

<script type="math/tex; mode=display">\min_{\theta, \theta'} {1\over n}\sum_{i=1}^nL(x_i, g(f(x_i))).</script>

<p>損失関数 $L$ としては、入力がバイナリ値ならば交差エントロピー誤差、一般の実数値ならば二乗誤差を用いるのが一般的です。</p>

<p>AE自体は1980年代から知られていたようです(要出典)。
オリジナルのAEには ${d\le d_\textrm{h}}$ のときに恒等関数が最適になってしまうという問題点があります。
この問題を回避するために $d&gt;d_\textrm{h}$ としたり（ボトルネック型のAE）、正則化項を加えたものを用いたりしていました。</p>

<h2 id="denoising-autoencoder-dae">Denoising Autoencoder (DAE)</h2>

<p>DAE [1]は正則化項とは異なるアプローチで2008年にPascal Vincentらが提案したAEの亜種です。
入力の一部を破壊することで、恒等関数が最適でないような問題に変形します。</p>

<p>入力ベクトルの一部にノイズを加える破壊分布(corruption distribution) $\mathcal C(\tilde X\vert X)$ を考えます。
破壊分布としてはガウスノイズや、成分をランダムに選んで0や1に潰す塩胡椒ノイズ(salt and pepper noise)などが用いられます。</p>

<p>入力 $X=x$ に対して $\mathcal C(\tilde X|X)$ から $\tilde X$ をサンプリングして、そこからエンコーダ・デコーダを使って入力 $X$ を復元します。
入力データを生成する分布を $\mathcal P(X)$ とします。
このときDAEは以下の期待値最適化問題として書かれます。</p>

<script type="math/tex; mode=display">\min_{\theta, \theta'} {\mathbb E}_{X\sim\mathcal P(X), \tilde X\sim\mathcal C(\tilde X\vert X)}L(X, g(f(\tilde X))).</script>

<p>これは同時分布 $\mathcal P(X, \tilde X)=\mathcal P(X)\mathcal C(\tilde X\vert X)$ 上の確率的最適化問題なので、確率的勾配降下法 (SGD)などで最適化できます。</p>

<p>関数 $g\circ f$ は破壊された（ノイズの乗った）入力 $\tilde X$ をもとの入力 $X$ に復元するように学習されるので、提案者であるP. VincentはこれをDenoising Autoencoderと名づけました。</p>

<h3 id="dae">DAEの様々な解釈</h3>

<p>DAEの見た目はAEのちょっとした亜種ですが、確率的な操作が関わることで多様な解釈を生み出しています。
例えばVincentの元論文ではDAEに対するいくつかの解釈が述べられています。</p>

<ul>
  <li><strong>多様体学習</strong>
高次元空間 $\mathbb R^d$ 上のデータ分布 $\mathcal P(X)$ は低次元多様体に埋まっているとする仮説があります。
DAEは多様体から少しずれた位置にある $\tilde X$ を多様体上に引き戻すように学習されるので、この低次元多様体を学習していることに相当します。
DAEはさらに、多様体の法線方向へのずれに対してロバストな写像を学習すると考えることができます。
この考え方はContractive Autoencoderへと受け継がれています[2]。</li>
  <li><strong>生成モデル</strong>
バイナリ値の入力の場合を考えます。
$f, g$ を用いて二つの生成モデルを考えます。
    <ul>
      <li>エンコーディングのモデル $q^0(X, \tilde X, Y)=q^0(X)\mathcal C(\tilde X\vert X)\delta_{f_\theta(\tilde X)}(Y)$。ここで $q^0(X)$ は経験分布、$\delta$ は添字を平均とするデルタ分布。</li>
      <li>デコーディングのモデル $p(X, \tilde X, Y)=p(Y)p(X\vert Y)\mathcal C(\tilde X\vert X)$。ここで $p(Y)$ は $[0, 1]^{d_\text{h}}$ 上の一様分布、$p(X\vert Y)$ は $g_{\theta’}(Y)$ を平均とするベルヌーイ分布。</li>
    </ul>

    <p>このとき、ごにょごにょ計算すると、DAEの誤差最小化問題は二つのモデルの間の交差エントロピー $\mathbb E_{q^0(\tilde X)}[-\log p(\tilde X)]$ の変分下界最大化と等価であることが示せます。</p>
  </li>
  <li><strong>情報理論</strong>
DAEの最小化問題は他にも入力層 $X$ と隠れ層 $Y$ の相互情報量 $I(X; Y)$ の下界を最大化していることとも等価になりそうです。</li>
</ul>

<p>またVincentの2011年の論文[3]ではスコアマッチングとの関係が示されています。
<strong>スコアマッチング (Score Matching)</strong> [4]とは分配関数を計算しないで確率モデルのパラメータを推定するための手法の一つです。
確率分布 $p(\xi; \theta)$ のスコア関数を $\psi(\xi; \theta)=\nabla_{\xi}\log p(\xi; \theta)$ と定義します<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>。
つまりスコア関数は入力に対する対数密度の勾配です。
これは真の分布 $p_x(\xi)$ についても考えることができます: $\psi_x(\xi)=\nabla_{\xi}\log p_x(\xi)$。
これら2つのスコア関数の期待二乗距離を最小化するのがスコアマッチングです。</p>

<script type="math/tex; mode=display">\min_{\theta}\frac12\int\lVert\psi(\xi; \theta)-\psi_x(\xi)\lVert^2dp_x(\xi).</script>

<p>$\psi_x$ が計算できないように見えますが、ごにょごにょ計算するとこれをモデルの2階微分を使った式で置き換えることができます。
スコアマッチングは入力に対する対数勾配を取ることで、分配関数を消しているところが特徴です（分解関数は入力に依存しないため、入力で微分すると消えます）。</p>

<p>[3]では、入力が実変数で損失関数が二乗誤差の場合のDAEが、スコアマッチングにおいて真の分布 $p_x$ の代わりに各訓練データを平均とする成分（パーゼン窓）からなる混合ガウス分布を用いたものと等価であることを示しています。
その過程でtied weightsが自然に導出されます。
またDAEの目的関数に対応するエネルギー関数も導出されています。</p>

<script type="math/tex; mode=display">E(x; \theta)=-{1\over\sigma^2}\left(\langle c, x\rangle-\frac12\lVert x\lVert^2+\sum_{j=1}^{d_\text{h}}\text{softplus}(\langle W_j, x\rangle+b_j)\right).</script>

<p>ここで $\sigma^2$ はパーゼン窓の分散パラメータ、$\text{softplus}(t)=\log(1+e^t)$、$W_j$ は重み行列 $W$ の $j$ 行目、$b_j$ はバイアス $b$ の第 $j$ 成分です。
エネルギー関数はRBMのものによく似ています（係数が少し違います）。</p>

<h2 id="dae-1">一般化DAE</h2>

<p>これまでに述べたDAEの特徴的な性質は、破壊された入力からもとの入力を復元するという操作から導き出されるものでした。
復元するための関数を2層のニューラルネットに限定せず、また入力や隠れ変数の種類（バイナリか実数かなど）や破壊分布や損失関数の種類に依らない、一般的な確率モデルとして捉え直したものがYoshua Bengioによって今年のNIPSで提案されています [5]。
さらにそのDAEが定めるマルコフ連鎖によって、真のデータ生成分布 $\mathcal P(X)$ 全体を推定することができると主張しています。</p>

<p>真のデータ生成分布を $\mathcal P(X)$、破壊分布を $\mathcal C(\tilde X\vert X)$、パラメータ $\theta$ を持つDAEのモデルを $P_\theta(X\vert\tilde X)$ とします。
このモデル $P_\theta(X\vert\tilde X)$ としては、もとのDAEの定式化でいえば $g(f(\tilde X))$ を平均とするベルヌーイ分布やガウス分布などが考えられます（入力変数の種類によって適切なものを使います）。
このとき一般化DAEでは同時分布 $\mathcal P(X, \tilde X)=\mathcal P(X)\mathcal C(\tilde X\vert X)$ 上の負の期待対数尤度 $\mathcal L(\theta)=-\mathbb E_{\mathcal P(X, \tilde X)}\log P_\theta(X\vert\tilde X)$ を最小化します。
実際には有限個の訓練データしか使えないので、正則化項を加えた次式を最小化します。</p>

<script type="math/tex; mode=display">\mathcal L_n(\theta)=\frac1n\sum_{X\sim\mathcal P(X), \tilde X\sim\mathcal C(\tilde X\vert X)}\lambda_n\Omega(\theta, X, \tilde X)-\log P_\theta(X\vert\tilde X).</script>

<p>ここで $\Omega$ は正則化項、$n$ は訓練データ数、つまり上式の総和の項数です。
この関数 $\mathcal L_n(\theta)$ を最小化するパラメータを $\theta_n$ とおきます。
ここで正則化の係数 $\lambda_n$ は $\lambda_n\to0\,(n\to\infty)$ となるように選びます。
よって$\mathcal L_n\to\mathcal L$ となります。</p>

<p>このモデルをもとに、$X$ と $\tilde X$ を交互に生成するマルコフ連鎖を考えます。</p>

<script type="math/tex; mode=display">X_t\sim P_\theta(X\vert\tilde X_{t-1}),\,\tilde X_t\sim\mathcal C(\tilde X\vert X_t).</script>

<p>$\theta=\theta_n$ の場合を考えます。
これを $X_t$ に関するマルコフ連鎖と考えたときの遷移作用素は $\tilde X$ を積分消去して $T_n(X_t\vert X_{t-1})=\int P_{\theta_n}(X_t\vert\tilde X)\,\mathcal C(\tilde X\vert X)\,d\tilde X$ となります。
$T_n$ が定めるマルコフ連鎖が漸近分布を持つとは限りませんが、持つ場合にはそれを $\pi_n$ とおきます。
また $T=\lim_{n\to\infty}T_n$ とします。
このとき、[5]では以下の定理が成り立つことを示しています。</p>

<p><strong>定理1</strong> $P_{\theta_n}(X\vert\tilde X)$ が $\mathcal P(X\vert\tilde X)$ のconsistent estimatorで、かつ $T_n$ が定めるマルコフ連鎖がエルゴード的なら、$n\to\infty$ において $\pi_n(X)$ は $\mathcal P(X)$ に収束する。</p>

<p>つまり上のマルコフ連鎖が既約（任意の状態間を移動できる）で非周期的ならば $P_{\theta}(X\vert\tilde X)$ を正しく推定することで真のデータ生成分布が得られるということを言っています。</p>

<p>[5]が重要だと主張しているポイントは、破壊された入力 $\tilde X$ に対する $\mathcal P(X\vert\tilde X)$ を推定する方が $\mathcal P(X)$ を直接推定するより簡単だという点です。
$\mathcal P(X)$ は一般に多峰性の複雑な形になりますが、$\mathcal P(X\vert\tilde X)$ は破壊分布 $\mathcal C(\tilde X\vert X)$ が十分局所的なら、ほとんど一つのモード（峰）だけが支配的になり、推定が容易になると言っています。</p>

<h3 id="walkback">Walkbackアルゴリズム</h3>

<p>直前の段落で述べたポイントでは $\tilde X$ が $X$ に十分近ければ推定できるという話でしたが、上のマルコフ連鎖においてギブスサンプリングを回しているうちに $\tilde X_t$ がもとの $X_0$ から離れていってしまう可能性があります。
特にデータ多様体から離れた位置ではモデルがあまり学習されないため、一度多様体の近くを離れてしまうと誤ったモード(spurious modes)に吸い込まれていく可能性があります。
[5]ではこれに対処するために、複数回ギブスサンプリングを回して得られた $\tilde X_t$ についても訓練データに入れる（つまり $P_\theta(X_0\vert\tilde X_t)$ も最大化する）という方法を提案しています。
これを、遠くに行ってしまったサンプルをもとの場所に引き戻す、という意味で<strong>Walkbackアルゴリズム</strong>と呼んでいます。
サンプリングを回す回数は適当に決めます（[5]では幾何分布からサンプリングして決めています）。
WalkbackアルゴリズムはもとのDAEと同じ分布を学習することも示されていますが、もとのDAEよりも経験的に性能が良いそうです。</p>

<p>Walkbackアルゴリズムは、モデルから生成されたサンプルを訓練データに使うという意味で、RBMに対するContrastive Divergence（特にCD-$k$アルゴリズム）と似ています。</p>

<h2 id="generative-stochastic-network-gsn">Generative Stochastic Network (GSN)</h2>

<p>Yoshua Bengioは一般化DAEをさらに一般化した枠組みを提案しています[6]。
一般化DAEの枠組みでは潜在変数が含まれておらず、すべて $P_\theta(X\vert\tilde X)$ に込められていました。
このモデルに複雑な構造（例えば深層モデル）を持たせるために、一般化DAEに潜在変数を加えたものがGSNです。</p>

<p>GSNでは、一般化DAEにおけるマルコフ連鎖が潜在変数 $H_t$ を用いて次のように置き換えられます。</p>

<script type="math/tex; mode=display">H_{t+1}\sim P_{\theta_1}(H\vert H_t, X_t),\,X_{t+1}\sim P_{\theta_2}(X\vert H_{t+1}).</script>

<p>ここで $H_{t+1}$ を生成する式はノイズ変数 $Z_t$ を使って $H_{t+1}=f_{\theta_1}(X_t, Z_t, H_t)$ と表されるとします。
一般化DAEの場合にはこの $P_{\theta_1}(H\vert H_t, X_t)$ が破壊分布 $\mathcal C(\tilde X_t\vert X_t)$ になっています（つまり $\tilde X_t$ が潜在変数となります）。</p>

<p>[6]ではGSNにおいても一般化DAEと同様の収束性が成り立つことを示しています。
つまり上のマルコフ連鎖に対しても、一般化DAEと同じような仮定のもとで漸近分布がデータ生成分布 $\mathcal P(X)$ に収束します。</p>

<p>GSNではギブスサンプリングの過程で潜在変数 $H_t$ を記憶することができる点が一般化DAEとの重要な違いのようです。
論文ではDeep Boltzmann Machineのサンプリングの計算グラフと同じように計算が進むようなGSNを例として作っています（[6]Figure 3）。
これは隠れ層が3層あるモデルで、奇数目の層と偶数目の層を交互にサンプリングします。
入力層をサンプリングしたあとには一般化DAEのマルコフ連鎖同様にノイズを加えます。
また、隠れ層の各ユニットは活性化関数の前後にノイズを入れているようです（確率的ニューロンと呼んでいます[7]）。</p>

<p>Walkbackアルゴリズムにより、複数ステップ後の復元結果を学習に使うことができます。
DBMと異なりGSN全体は（確率的な）ニューラルネットなので、この復元層からの学習には誤差逆伝播法が使えます。
これにより、GSN全体は生成モデルでありながら、深いモデルでも教師あり学習の種々のテクニックが利用できるのが利点です。
例えば畳み込みレイヤーやプーリング、AdaGrad、Dropoutなどがそのまま使えます。</p>

<p>また、GSNは入力に欠損値がある場合にも使え、観測されている入力値から欠損値を補完することができます。
観測されている入力値 $X^{(s)}$ を固定した状態でギブスサンプリングを回せば $\mathcal P(X^{(-s)}\vert X^{(s)})$ からサンプリングできます。
ここで $X^{(-s)}$ は残りの入力変数です。</p>

<h2 id="section">まとめ</h2>

<p>今日はDenoising Autoencoderの定式化と様々な解釈から2段階の一般化（一般化DAEとGenerative Stochastic Network）を紹介しました。
生成モデルの解析周りは個人的に慣れない分野で、今回の話の中にも咀嚼できていない点が多々ありますが、DAEという一つのモデルからこれだけ多様な話題が出てくるのは面白いなあと思っています。
一般化について、特にGSNについては、具体的な応用が来年には出てくるのかなと想像しています。
特に教師あり学習のテクニックが使えるというのが気になっています。
これがブレイクスルーになるかはわかりませんが、これからも追っていきたいです。</p>

<p>MLACについては他の記事も読ませていただいており、今回の記事執筆自体も自分自身勉強になりました。
MLACの著者の皆様、特に企画をしてくださった@naoya_tさん、ありがとうございました。</p>

<h2 id="section-1">参考文献</h2>

<p>[1] Pascal Vincent, Hugo Larochelle, Yoshua Bengio and Pierre-Antoine Manzagol. <a href="http://www.iro.umontreal.ca/~vincentp/Publications/vincent_icml_2008.pdf">Extracting and Composing Robust Features with Denoising Autoencoders</a>. <em>Proc. of ICML, 2008</em>.<br />
[2] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot and Yoshua Bengio. <a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf">Contractive Auto-Encoders: Explicit Invariance During Feature Extraction</a>. <em>Proc. of ICML, 2011</em>.<br />
[3] Pascal Vincent. <a href="http://www.iro.umontreal.ca/~vincentp/Publications/DenoisingScoreMatching_NeuralComp2011.pdf">A Connection Between Score Matching and Denoising Autoencoders</a>. <em>Neural Computation, 2011</em>.<br />
[4] Aapo Hyv̈arinen. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/Hyvarinen05.pdf">Estimation of Non-Normalized Statistical Models by Score Matching</a>. <em>Journal of Machine Learning Research 6, 2005</em>.<br />
[5] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. <a href="http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models">Generalized Denoising Auto-Encoders as Generative Models</a>. <em>NIPS, 2013</em>.<br />
[6] Yoshua Bengio. <a href="http://arxiv.org/abs/1306.1091">Deep Generative Stochastic Networks Trainable by Backprop</a>. <em>arXiv:1306.1091, 2013</em>.<br />
[7] Yoshua Bengio. <a href="http://arxiv.org/abs/1305.2982">Estimating or Propagating Gradients Through Stochastic Neurons</a>. <em>arXiv:1305.2982, 2013</em>.</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>といっても僕自身これらを自分で実装して動かしたわけではなく、論文追っかけてるだけなので誰かの受け売り状態なのですが。。。<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>この定義は統計学におけるもともとの「スコア関数』の定義とは異なるそうです。もともとは確率モデルの対数密度勾配を、入力変数ではなくモデルのパラメータについて取ったものがスコア関数でした。<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Juliaを使ってみた]]></title>
    <link href="http://beam2d.github.io/blog/2013/09/20/itq-by-julia/"/>
    <updated>2013-09-20T22:38:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2013/09/20/itq-by-julia</id>
    <content type="html"><![CDATA[<p>ブログ遅れちゃいましたが。
今週のはじめにJuliaで書いたPCA hashとITQの実装を公開しました。</p>

<p><a href="https://github.com/beam2d/julia-pcahash">https://github.com/beam2d/julia-pcahash</a></p>

<p>Juliaというのは数値計算・科学計算がメインターゲットの新しい言語です。
つい最近触り始めたんですが、今のところ割りと良好です。
julia-pcahashは勉強用に書きました。</p>

<!-- more -->

<p>Juliaには以下の様な特徴があります（僕の目にぱっとついたところだけで）。</p>

<ul>
  <li>多次元配列が簡単に扱えて線形代数計算が充実している(MATLABやnumpy/scipyのノリ)</li>
  <li>文法はMATLABとRubyとPythonを混ぜたような感じ</li>
  <li>LLVMベースで、JITが走る</li>
  <li>動的型付けで、型アノテーションや多相型があり、型推論による静的最適化が走る</li>
  <li>他の科学技術用言語にくらべて数値計算以外の処理が比較的速い</li>
  <li>Juliaの構文自体をデータ構造として扱えて、マクロが書ける(LISPみたいに<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>)</li>
</ul>

<p>Juliaでコード書いてみてわかったことが以下のとおり。</p>

<ul>
  <li>ファイルの読み書きは割りと簡単にできる</li>
  <li>行列計算はMATLABやnumpy/scipyみたいに書けるが細かい所はいろいろ違う</li>
  <li>1-originはやっぱり不便</li>
  <li>マクロが結構便利に見える</li>
  <li>BitArrayは2階以上でもびっちり詰まっていて、そこを意識しないと性能劣化する</li>
  <li>引数・戻り値が簡単な型（行列含む）なら、Cのコード呼び出し(ccall)はとても簡単
    <ul>
      <li>今回、ハミング距離の計算をSSEでやるコードをCで書いて、それを呼び出した<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></li>
    </ul>
  </li>
  <li>標準ライブラリはまだ痒いところにちょっと手が届かないレベル</li>
  <li>パッケージは盛んに開発されている雰囲気</li>
  <li>起動が重い</li>
</ul>

<p>速度もそこそこ速いように思います。
そのうちちゃんとC++やPythonと比較したいところ。
もうちょっと使ってみようと思います。</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>LISP詳しくないですがそういう感じらしいです、homoiconicというらしい。ちなみにJuliaのパーザはSchemeで書かれています。<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Juliaで直接 nnz(x $ y) のように書くこともできますが、Cで書いたSSE4のコードの方が手元の環境では3倍ほど速かった<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Isotropic Hashingと線形ハッシュ学習]]></title>
    <link href="http://beam2d.github.io/blog/2013/02/23/isohash/"/>
    <updated>2013-02-23T14:53:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2013/02/23/isohash</id>
    <content type="html"><![CDATA[<p>今日は前回に引き続きNIPS2012の論文を紹介します。
前回紹介した<a href="http://beam2d.github.io/blog/2013/01/26/super-bit-lsh/">Super-Bit LSH</a>はデータ非依存のハッシュ法でしたが、Isotropic Hashing (以下IsoHash)はデータを使った教師なしハッシュ学習です。</p>

<p>Weihao Kong and Wu-Jun Li. <a href="http://www.cs.sjtu.edu.cn/~liwujun/paper/NIPS12-IsoHash.pdf">Isotropic Hashing</a>. NIPS 2012.</p>

<!-- more -->

<p>IsoHashはPCAHがベースになっています。
ハッシュの学習ではよく、次元削減してから各次元を正か負かで $\pm1$ に潰す、ということをします<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。
PCAHはこの次元削減としてPCAを用いるというものです。
非常にお手軽ですが、PCA軸は次元によって情報量に偏りがあるのに等しく1bitずつに潰してしまうので、近傍探索の精度がよくありません。
そこでIsoHashでは、PCAで次元削減したあとに、偏りがならされるように回転してから潰します。
偏りの指標としては、データ集合に対する各次元の分散を用います。</p>

<p>学習に用いるデータ行列を $X\in\mathbb R^{d\times n}$ とおき、PCAで次元削減する行列を $W\in\mathbb R^{d\times m}$ とおきます。
このとき、直交行列 $Q\in\mathcal O(m)$ を用いて、 $Q^\top W^\top XX^\top WQ$ の対角成分がすべて等しくなるようにします（対角成分の値はPCAの固有値の平均になります）。
$Q$ を求めたいですが、IsoHashでは $Q^\top W^\top XX^\top WQ$ という形の行列の集合と、対角成分がすべて等しい行列の集合、の共通部分を求めることにします。
この問題が解ければ、固有値分解をして $Q$ が取り出せます。
論文では、2つの集合間を交互に射影するLift and Projectionという方法と、直交群上で対角成分の二乗和誤差を最小化する問題として勾配を用いて解くGradient Flowという方法の2つを提案しています。
実験を見る限りどっちが優れてるということもないようです。</p>

<p>PCAHを改良した線形ハッシュ学習としては、IsoHashの前にもIterative Quantization (ITQ <sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>)という手法が提案されていました。
ITQでは、 $\pm1$ に潰す前の各次元の値ができるだけ $\pm1$ に近くなるように線形変換を行います。
この最適化はk-Meansのような感じで線形変換の更新とハッシュ値の更新を交互に行うのですが、ここが重いのが欠点でした。
IsoHashはITQと精度的には互角ぐらいのようで、かつ学習がそこそこ速い（特にPCAしたあとはデータ数によらない）のが売りのようです。</p>

<p>あれこれ書いた上でちゃぶ台ひっくり返す感じなのですが、各次元の偏りをならす一番お手軽な方法は、ランダム行列をかけるというやり方です。
ITQの論文ではPCA後にランダム行列をかけてから潰す、という方法も比較してますが、実はこれで結構いい精度が出ちゃいます。
なので現時点では、Practicalにはランダム行列を使うのが良いかもしれません。</p>

<p>あと、論文に沿ってPCAを使って書いてきましたが、非線形な次元削減の上にも乗っかるはずです。</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>事前にデータを平均が0になるように正規化しておくことが多いです。<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Yunchao Gong and Svetlana Lazebnik.
<a href="http://www.cs.illinois.edu/homes/slazebni/publications/cvpr11_small_code.pdf">Iterative Quantization: A Procrustean Approach to Learning Binary Codes</a>.
CVPR, 2011.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Super-Bit LSH]]></title>
    <link href="http://beam2d.github.io/blog/2013/01/26/super-bit-lsh/"/>
    <updated>2013-01-26T13:54:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2013/01/26/super-bit-lsh</id>
    <content type="html"><![CDATA[<p>今日は、すこし前に読んだSuper-Bit LSH (SB-LSH)という手法を簡単に紹介します。
これは角類似度<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>に対するランダム射影LSH<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>の改良版です。</p>

<p>Jianqiu Ji, Jianmin Li, Shuicheng Yany, Bo Zhang, Qi Tianz.<br />
Super-Bit Locality-Sensitive Hashing. NIPS 2012.</p>

<!-- more -->

<p>ランダム射影LSHの解説は<a href="http://blog.jubat.us/2012/05/17-web.html" title="第17回 データマイニング+WEB＠東京で発表しました | Jubatus Blog">海野さんのスライド(29-30ページ目)</a>がわかりやすいです。
ランダム射影LSHを使うと、角類似度の不偏推定量が得られます。</p>

<p>Super-Bit LSHではこれをNビットごとにグループ分けして、各グループ内の射影ベクトルを直交させるというものです。
直交化はたとえばGram-Schmidtの直交化法を使います。
こうすると、推定量の分散がただのランダム射影よりも小さくなるということです。
論文ではこのNビットのグループをSuper Bitと呼んでいます。</p>

<p>論文中の実験を見る限り、特に直角に近い角度の推定が正確になるようです。
これは近傍探索に使うときにはあまり問題にならないですが、別の用途で純粋に角度の推定値が欲しい時や、Active Learningに用いるとき<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>には役立ちそうです。</p>

<p>ただ、次元がとても高い場合、ランダムなベクトル同士がほとんど直交する（大数の法則）ので、あまり効果はないかもね、と書かれています。
論文中の実験ではSIFT特徴量(128次元)とBoVW(3125次元)を使っているようで、これくらいだと有効みたいです。
また、自然言語処理などでよく事前に次元数がわからない場合がありますが、そういう場合も使えなさそうです。</p>

<p>ランダム射影LSHはLSHの中でもかなり古典的なアルゴリズムですが、データ非依存という設定でその精度が改良されるのはおそらく初めてだと思います（論文中でもそう書かれています）。
今まではいろんな種類の類似度に対するLSHの開発が盛んでしたが、そろそろ既存のLSHを改良する</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>2つのベクトル $x, y$ に対して $1-{1\over\pi}\cos^{-1}\left({x\cdot y\over\lVert x\lVert\,\lVert y\lVert}\right)\in[0, 1]$ を角類似度(angular similarity)と呼びます。
Cosine類似度やArccos類似度と呼ばれることもあります。<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>SimHash、Cosine LSH、Arccos LSHなどいろんな呼ばれ方をしています。
論文中ではSRP-LSH (Sign-random-projection LSH) と呼んでいます。<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>重みベクトルとできるだけ直交する教師データを選びたいときなど。<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[初投稿]]></title>
    <link href="http://beam2d.github.io/blog/2013/01/20/first-post/"/>
    <updated>2013-01-20T10:33:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2013/01/20/first-post</id>
    <content type="html"><![CDATA[<p>Bloggerを使っていたのですが書きづらかったので、Octopressを使ってみました。
OctopressはWindowsでも割りと簡単でした。
文字コードで一瞬ハマりましたが、環境変数にLANG=ja_JP.UTF-8を指定したら動きました。
今のところ良い感じです。</p>
]]></content>
  </entry>
  
</feed>
