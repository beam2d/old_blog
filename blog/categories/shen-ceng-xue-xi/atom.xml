<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 深層学習 | @beam2d]]></title>
  <link href="http://beam2d.github.io/blog/categories/shen-ceng-xue-xi/atom.xml" rel="self"/>
  <link href="http://beam2d.github.io/"/>
  <updated>2015-02-02T11:52:06+09:00</updated>
  <id>http://beam2d.github.io/</id>
  <author>
    <name><![CDATA[Seiya Tokui]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[NIPS2014読み会で深層半教師あり学習の論文を紹介しました]]></title>
    <link href="http://beam2d.github.io/blog/2015/01/24/ssl-deep/"/>
    <updated>2015-01-24T15:30:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2015/01/24/ssl-deep</id>
    <content type="html"><![CDATA[<p>今週の火曜日 (1/20) に東大で <a href="http://connpass.com/event/10568/">NIPS2014 読み会</a> が開かれました．
NIPS 自体の参加者数が増えているのと同様に，読み会も去年にくらべてさらに多くの人が集まりました．
その中で僕もひとつ論文を紹介しました．</p>

<!-- more -->

<iframe src="http://beam2d.github.io//www.slideshare.net/slideshow/embed_code/43686520" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""> </iframe>
<div style="margin-bottom:5px"> <strong> <a href="http://beam2d.github.io//www.slideshare.net/beam2d/semisupervised-learning-with-deep-generative-models" title="論文紹介 Semi-supervised Learning with Deep Generative Models" target="_blank">論文紹介 Semi-supervised Learning with Deep Generative Models</a> </strong> from <strong><a href="http://beam2d.github.io//www.slideshare.net/beam2d" target="_blank">Seiya Tokui</a></strong> </div>

<p>紹介した論文の著者は，スライド中にも出てくる変分 AutoEncoder の考案者です．
変分 AE では，生成モデルと認識モデルをそれぞれニューラルネットで定義して，確率変数としてそれらの出力をパラメータとする正規分布を使いました．
生成モデルを認識モデルで近似したときの変分下界は，認識モデルに関する期待値の形をしています．</p>

<p>このように，最適化の対象となる分布に関する期待値の最適化は，一般には REINFORCE アルゴリズムによる勾配法を使います．
REINFORCE アルゴリズムは，期待値を積分で書いた時に，積の微分を使って勾配を計算し，それを対数微分の公式 $x’ = (\log x)’ x$ をつかって $q(\mathbf z | \mathbf x)$ に関する期待値の形に戻すことで導出できます．
これをサンプリングにより近似しますが，この方法で得られる勾配のサンプルは大きな分散を持ち，学習が非常に遅いという問題があります．
変分 AE の学習アルゴリズム（スライド中で SGVB および SBP と書いたもの）は，正規乱数を用いた場合にこの分散の問題を解決した手法です．</p>

<p>今回紹介した論文は，この変分 AE の半教師あり学習への応用です．
かなり素直な拡張で，査読でも straight forward ではないかと指摘されていました．
個人的には実験が面白いのと，生成・認識モデルはもっと知られても良いと思ったので，紹介させていただきました．</p>

<p>さて，この生成モデルと認識モデルを同時に学習するという話は，僕の知る限りでは <a href="https://www.cs.toronto.edu/~hinton/helmholtz.html">1994～1996 年あたりに Hinton がやっていた仕事</a>が源流のようです．
当時 Hinton は，各変数が二値を取るような多層のベイジアンネット（Sigmoid Belief Net などとも呼ばれます）をおもな対象としていました．
生成モデル $p$ と認識モデル $q$ の間で，$KL(q \| p)$ を最小化するような学習法を Helmholz Machine と呼んでいます．
Helmholz Machine の学習方法として 1995 年に Hinton が考案したのが Wake-Sleep アルゴリズムです．
このアルゴリズムに対しては，最近 <a href="http://arxiv.org/abs/1406.2751">Reweighted Wake-Sleep</a> という発展形も提案されています．
変分 AE では，Sigmoid Belief Net にかわって，潜在変数と観測変数の関係を決定的な多層パーセプトロンで与えています．</p>

<p>深層学習の文脈で生成モデルというと Deep Belief Net や Deep Boltzmann Machine が主流です．
これらは非常に高い表現力を持っていて，おもしろい実験結果がたくさんあります．
一方，これらのモデルは計算において扱いづらいのが問題です．
たとえば，Deep Belief Net は，単体では推論できません．
Deep Boltzmann Machine は推論が可能ですが，MCMC を必要とし，計算コストがかかります．</p>

<p>変分 AE のような生成・認識モデルは，推論が簡単なのがウリです．
生成も認識もただの伝承サンプリングですみ，MCMC が必要ありません．
MCMC が必要ない深層生成モデルとしては，ほかに NADE など，Sigmoid Belief Net を拡張する研究があります．</p>

<p>もうひとつ，伝承サンプリングが可能な新しいモデルとして <a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets">Generative Adversarial Nets</a> というのもあります．
このモデルは，ニューラルネットで定義した生成モデルと，やはりニューラルネットで定義した識別モデルからなります．
識別モデルは，与えられたデータが真のデータ分布（または経験分布）から生成されたものなのか，あるいは学習中の生成モデルから生成されたものなのかを判別します．
学習のときにはこれらを同時に最適化します．
GAN は，学習のときに推論が必要ないというおもしろい特徴をもっています．
一方で，裏を返せば GAN は推論器を学習しないため，推論をしたかったら別に推論モデルを学習させる必要があります．</p>

<p>変分 AE や GAN では，深さを実現しているのは決定的なニューラルネット（多層パーセプトロン）です．
これを誤差逆伝搬による勾配法で学習します．
よって，これらの手法において，ニューラルネット部分の設計や最適化には，教師あり学習で培われてきた技術（dropout など）がほぼそのまま転用できます．
このことは，データの規模が大きくなってきたときに大きなアドバンテージになるでしょう．</p>

<p>さて，そういうわけで，伝承サンプリングできる深層生成モデルの学習は少しずつ盛り上がっていて，個人的に注目している分野です．
まだ小規模なデータでの実験にとどまっていますが，そろそろ大規模データでの検証もされていくのではないかと期待しています．</p>

<p>ちなみに，この論文の著者は <a href="http://arxiv.org/abs/1412.6980">Adam</a> という最適化手法を ICLR2015 に提出しています．
これはモーメンタム法を RMSprop に組み込んだような手法です．
今回読み会で紹介した論文でも「モーメンタム法を使った RMSprop」を使っていましたが，それをちゃんとまとめたものが Adam ではないかと思います．
凸な場合のリグレット解析もされていて，実験結果も安定して良いようで，こちらも気になります．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Softplus関数]]></title>
    <link href="http://beam2d.github.io/blog/2014/03/02/softplus/"/>
    <updated>2014-03-02T22:38:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2014/03/02/softplus</id>
    <content type="html"><![CDATA[<p>今日、Caffeというナウいニューラルネット実装を読んでいたら次の行で？？？となりました。</p>

<p><a href="https://github.com/BVLC/caffe/blob/v0.9/src/caffe/layers/bnll_layer.cu#L20">https://github.com/BVLC/caffe/blob/v0.9/src/caffe/layers/bnll_layer.cu#L20</a></p>

<p>数式で書くと（logは自然対数）</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

f(x)=\begin{cases}
  x+\log(1+e^{-x}) & \text{if}\,x > 0, \\
  \log(1+e^x) & \text{otherwise}.
\end{cases}
 %]]&gt;</script>

<p>もっとオシャレに書くと</p>

<script type="math/tex; mode=display">f(x)=\max(0, x)+\log(1+e^{-\vert x\vert}).</script>

<p>これが数値計算的なテクニックなのか、近似も含んでるのか、僕が知らないactivation関数なのかパッと見判断つかなかったのですが、微分してみたら両方シグモイド関数になりました（で、よく見たらすぐ下に導関数の実装も書いてあった）。
まず滑らかなのかこれ、というところでちょっと驚く。</p>

<p>さて、シグモイドを導関数に持つのは、softplusと呼ばれる関数です（これも忘れててしばらく考えてた）。</p>

<script type="math/tex; mode=display">\text{softplus}(x)=\log(1+e^x).</script>

<p>これはRectified linear Unit (ReLU) とかpositive partと呼ばれる式 $\max(0, x)$ と似た性質を持っていて、かつ滑らかで勾配が（厳密には）消えない関数としてニューラルネット界隈で時々使われます。
つまり上の式はsoftplus関数なのでした。</p>

<p>実際、softplus関数にlogsumexpでやるような大きなexpの追い出しをやると $x&gt;0$ における最初の式が導出できます。</p>

<script type="math/tex; mode=display">
\log(1+e^x)=\log(e^x(e^{-x}+1))=x+\log(1+e^{-x}).
</script>

<p>当たり前ですが頑張ればもとの（オシャレな方の）式からsoftplusを導出することもできます。positive partとnegative partが出てきてちょっと面白い（個人の感想です）。</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
\max(0, x)+\log(1+e^{-\vert x\vert})
&=\log(e^{\max(0, x)}(1+e^{-\vert x\vert}))\\
&=\log(e^{\max(0, x)}+e^{\max(0, x)-\vert x\vert})\\
&=\log(e^{\max(0, x)}+e^{\min(0, x)})\\
&=\log(1+e^x).
\end{align*}
 %]]&gt;</script>

<p>気づいてしまえばそうなんですが、最初softplus関数のことが頭になかったので、しばらく考えてしまいました。
数値計算テクむずい。</p>

<p>さて、するとこの式はReLUとsoftplus関数の差を表す式ということになります。</p>

<script type="math/tex; mode=display">\text{softplus}(x)-\max(0, x)=\log(1+e^{-\vert x\vert}).</script>

<p>右辺の関数 $\log(1+e^{-\vert x\vert})$ をプロットしてみると次のようになります。</p>

<p><img src="/images/softplus-relu.png" alt="Softplus(x) - ReLU(x)" /></p>

<p>式から明らかですが、僕のぱっと見の想像と違ってSoftplusとReLUの差は左右対称な形をしていました。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Denoising Autoencoderとその一般化]]></title>
    <link href="http://beam2d.github.io/blog/2013/12/23/dae-and-its-generalization/"/>
    <updated>2013-12-23T22:48:00+09:00</updated>
    <id>http://beam2d.github.io/blog/2013/12/23/dae-and-its-generalization</id>
    <content type="html"><![CDATA[<p><a href="http://qiita.com/advent-calendar/2013/machinelearning">Machine Learning Advenc Calendar 2013</a>の23日目担当の得居です。
<a href="https://preferred.jp">株式会社Preferred Infrastructure</a>で<a href="http://jubat.us">Jubatus</a>を作ったりしています。</p>

<p>今日は深層学習(deep learning)の話です。
深層学習はこの2年ほどで専門外の人にも知れ渡るほどに大流行しました。
データさえ大量にあればテクニック次第で他の手法を圧倒する性能を達成できることから、特に大量のデータを持つ大企業において大々的な参入が相次ぎました。</p>

<p>主に流行っているのは教師あり学習です。
補助として教師なし学習による事前学習(pretraining)も、特に音声認識のタスクにおいては行われているようですが、画像認識を中心に事前学習なしでもテクニック次第で学習できるという見方が強まっています。</p>

<p>一方で教師なしデータからの学習はブレイクスルー待ちといった雰囲気です。
Deep Belief NetworksやDeep Boltzmann Machinesなど「無名隠れ変数 (anonymous latent variables)」を用いた生成モデルが先行して詳しく解析されていますが、その学習は教師あり学習のようにはうまくいっていないのが現状です<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。</p>

<p>今日お話するDenoising Autoencoder (DAE)はそんな教師なし深層学習で用いられる手法・モデルの一つです。
RBM系の深層生成モデルとは別の系統になりますが、DAEをベースにした確率的な深層モデルの理論が数年遅れて最近発展してきているので、今日はその概要をまとめて書きたいと思います。
主にPascal VincentとYoshua Bengioによるこの5年間の研究の流れに沿って、以下の様な順に書きます。</p>

<ul>
  <li>Autoencoderの定式化</li>
  <li>DAEの定式化とそのオリジナルの解釈</li>
  <li>DAEのスコアマッチングとしての解釈</li>
  <li>DAEの確率モデルとしての一般化</li>
  <li>一般化DAEのためのWalkbackアルゴリズム</li>
  <li>隠れ変数を入れたさらに一般のモデルとその特徴</li>
</ul>

<p>分量の割に時間がかかっていないので、説明が変だったりまどろっこしい部分があるかもしれません。
気づいた点がございましたらぜひ<a href="https://twitter.com/beam2d">@beam2d</a>までお知らせください。</p>

<!-- more -->

<h2 id="autoencoder-ae">Autoencoder (AE)</h2>

<p>まずAEの定式化を紹介します。
入力層 $x\in\mathbb R^d$ に対して隠れ層 $y=f_\theta(x)=s(Wx+b)\in\mathbb R^{d_\text{h}}$ と復元層 $x^r=g_{\theta’}(y)=s_r(W’y+c)\in\mathbb R^d$ を定義します。
ただし $\theta=(W, b)$, $\theta’=(W’, c)$ は共に学習すべきパラメータです。
$s, s_r$ は活性化関数(activation function)と呼ばれます。
$s_r$ は入力がバイナリ値ならシグモイド関数、一般の実数値なら恒等関数にします。
$W’=W^\top$ という制約(tied weights)を置くと性能が良いことが知られています（解釈の一つが後ほど出てきます）。
関数 $f_\theta$ はエンコーダ、$g_{\theta’}$ はデコーダと呼ばれます。</p>

<p>こうして定義された復元層 $x^r$ が入力層 $x$ にできるだけ近くなるようにエンコーダとデコーダのパラメータ $\theta, \theta’$ を学習します。
訓練データ $\mathcal D={x_1, \dots, x_n}$ に対して損失関数 $L(x, x^r)$ の平均値を最小化します:</p>

<script type="math/tex; mode=display">\min_{\theta, \theta'} {1\over n}\sum_{i=1}^nL(x_i, g(f(x_i))).</script>

<p>損失関数 $L$ としては、入力がバイナリ値ならば交差エントロピー誤差、一般の実数値ならば二乗誤差を用いるのが一般的です。</p>

<p>AE自体は1980年代から知られていたようです(要出典)。
オリジナルのAEには ${d\le d_\textrm{h}}$ のときに恒等関数が最適になってしまうという問題点があります。
この問題を回避するために $d&gt;d_\textrm{h}$ としたり（ボトルネック型のAE）、正則化項を加えたものを用いたりしていました。</p>

<h2 id="denoising-autoencoder-dae">Denoising Autoencoder (DAE)</h2>

<p>DAE [1]は正則化項とは異なるアプローチで2008年にPascal Vincentらが提案したAEの亜種です。
入力の一部を破壊することで、恒等関数が最適でないような問題に変形します。</p>

<p>入力ベクトルの一部にノイズを加える破壊分布(corruption distribution) $\mathcal C(\tilde X\vert X)$ を考えます。
破壊分布としてはガウスノイズや、成分をランダムに選んで0や1に潰す塩胡椒ノイズ(salt and pepper noise)などが用いられます。</p>

<p>入力 $X=x$ に対して $\mathcal C(\tilde X|X)$ から $\tilde X$ をサンプリングして、そこからエンコーダ・デコーダを使って入力 $X$ を復元します。
入力データを生成する分布を $\mathcal P(X)$ とします。
このときDAEは以下の期待値最適化問題として書かれます。</p>

<script type="math/tex; mode=display">\min_{\theta, \theta'} {\mathbb E}_{X\sim\mathcal P(X), \tilde X\sim\mathcal C(\tilde X\vert X)}L(X, g(f(\tilde X))).</script>

<p>これは同時分布 $\mathcal P(X, \tilde X)=\mathcal P(X)\mathcal C(\tilde X\vert X)$ 上の確率的最適化問題なので、確率的勾配降下法 (SGD)などで最適化できます。</p>

<p>関数 $g\circ f$ は破壊された（ノイズの乗った）入力 $\tilde X$ をもとの入力 $X$ に復元するように学習されるので、提案者であるP. VincentはこれをDenoising Autoencoderと名づけました。</p>

<h3 id="dae">DAEの様々な解釈</h3>

<p>DAEの見た目はAEのちょっとした亜種ですが、確率的な操作が関わることで多様な解釈を生み出しています。
例えばVincentの元論文ではDAEに対するいくつかの解釈が述べられています。</p>

<ul>
  <li><strong>多様体学習</strong>
高次元空間 $\mathbb R^d$ 上のデータ分布 $\mathcal P(X)$ は低次元多様体に埋まっているとする仮説があります。
DAEは多様体から少しずれた位置にある $\tilde X$ を多様体上に引き戻すように学習されるので、この低次元多様体を学習していることに相当します。
DAEはさらに、多様体の法線方向へのずれに対してロバストな写像を学習すると考えることができます。
この考え方はContractive Autoencoderへと受け継がれています[2]。</li>
  <li><strong>生成モデル</strong>
バイナリ値の入力の場合を考えます。
$f, g$ を用いて二つの生成モデルを考えます。
    <ul>
      <li>エンコーディングのモデル $q^0(X, \tilde X, Y)=q^0(X)\mathcal C(\tilde X\vert X)\delta_{f_\theta(\tilde X)}(Y)$。ここで $q^0(X)$ は経験分布、$\delta$ は添字を平均とするデルタ分布。</li>
      <li>デコーディングのモデル $p(X, \tilde X, Y)=p(Y)p(X\vert Y)\mathcal C(\tilde X\vert X)$。ここで $p(Y)$ は $[0, 1]^{d_\text{h}}$ 上の一様分布、$p(X\vert Y)$ は $g_{\theta’}(Y)$ を平均とするベルヌーイ分布。</li>
    </ul>

    <p>このとき、ごにょごにょ計算すると、DAEの誤差最小化問題は二つのモデルの間の交差エントロピー $\mathbb E_{q^0(\tilde X)}[-\log p(\tilde X)]$ の変分下界最大化と等価であることが示せます。</p>
  </li>
  <li><strong>情報理論</strong>
DAEの最小化問題は他にも入力層 $X$ と隠れ層 $Y$ の相互情報量 $I(X; Y)$ の下界を最大化していることとも等価になりそうです。</li>
</ul>

<p>またVincentの2011年の論文[3]ではスコアマッチングとの関係が示されています。
<strong>スコアマッチング (Score Matching)</strong> [4]とは分配関数を計算しないで確率モデルのパラメータを推定するための手法の一つです。
確率分布 $p(\xi; \theta)$ のスコア関数を $\psi(\xi; \theta)=\nabla_{\xi}\log p(\xi; \theta)$ と定義します<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>。
つまりスコア関数は入力に対する対数密度の勾配です。
これは真の分布 $p_x(\xi)$ についても考えることができます: $\psi_x(\xi)=\nabla_{\xi}\log p_x(\xi)$。
これら2つのスコア関数の期待二乗距離を最小化するのがスコアマッチングです。</p>

<script type="math/tex; mode=display">\min_{\theta}\frac12\int\lVert\psi(\xi; \theta)-\psi_x(\xi)\lVert^2dp_x(\xi).</script>

<p>$\psi_x$ が計算できないように見えますが、ごにょごにょ計算するとこれをモデルの2階微分を使った式で置き換えることができます。
スコアマッチングは入力に対する対数勾配を取ることで、分配関数を消しているところが特徴です（分解関数は入力に依存しないため、入力で微分すると消えます）。</p>

<p>[3]では、入力が実変数で損失関数が二乗誤差の場合のDAEが、スコアマッチングにおいて真の分布 $p_x$ の代わりに各訓練データを平均とする成分（パーゼン窓）からなる混合ガウス分布を用いたものと等価であることを示しています。
その過程でtied weightsが自然に導出されます。
またDAEの目的関数に対応するエネルギー関数も導出されています。</p>

<script type="math/tex; mode=display">E(x; \theta)=-{1\over\sigma^2}\left(\langle c, x\rangle-\frac12\lVert x\lVert^2+\sum_{j=1}^{d_\text{h}}\text{softplus}(\langle W_j, x\rangle+b_j)\right).</script>

<p>ここで $\sigma^2$ はパーゼン窓の分散パラメータ、$\text{softplus}(t)=\log(1+e^t)$、$W_j$ は重み行列 $W$ の $j$ 行目、$b_j$ はバイアス $b$ の第 $j$ 成分です。
エネルギー関数はRBMのものによく似ています（係数が少し違います）。</p>

<h2 id="dae-1">一般化DAE</h2>

<p>これまでに述べたDAEの特徴的な性質は、破壊された入力からもとの入力を復元するという操作から導き出されるものでした。
復元するための関数を2層のニューラルネットに限定せず、また入力や隠れ変数の種類（バイナリか実数かなど）や破壊分布や損失関数の種類に依らない、一般的な確率モデルとして捉え直したものがYoshua Bengioによって今年のNIPSで提案されています [5]。
さらにそのDAEが定めるマルコフ連鎖によって、真のデータ生成分布 $\mathcal P(X)$ 全体を推定することができると主張しています。</p>

<p>真のデータ生成分布を $\mathcal P(X)$、破壊分布を $\mathcal C(\tilde X\vert X)$、パラメータ $\theta$ を持つDAEのモデルを $P_\theta(X\vert\tilde X)$ とします。
このモデル $P_\theta(X\vert\tilde X)$ としては、もとのDAEの定式化でいえば $g(f(\tilde X))$ を平均とするベルヌーイ分布やガウス分布などが考えられます（入力変数の種類によって適切なものを使います）。
このとき一般化DAEでは同時分布 $\mathcal P(X, \tilde X)=\mathcal P(X)\mathcal C(\tilde X\vert X)$ 上の負の期待対数尤度 $\mathcal L(\theta)=-\mathbb E_{\mathcal P(X, \tilde X)}\log P_\theta(X\vert\tilde X)$ を最小化します。
実際には有限個の訓練データしか使えないので、正則化項を加えた次式を最小化します。</p>

<script type="math/tex; mode=display">\mathcal L_n(\theta)=\frac1n\sum_{X\sim\mathcal P(X), \tilde X\sim\mathcal C(\tilde X\vert X)}\lambda_n\Omega(\theta, X, \tilde X)-\log P_\theta(X\vert\tilde X).</script>

<p>ここで $\Omega$ は正則化項、$n$ は訓練データ数、つまり上式の総和の項数です。
この関数 $\mathcal L_n(\theta)$ を最小化するパラメータを $\theta_n$ とおきます。
ここで正則化の係数 $\lambda_n$ は $\lambda_n\to0\,(n\to\infty)$ となるように選びます。
よって$\mathcal L_n\to\mathcal L$ となります。</p>

<p>このモデルをもとに、$X$ と $\tilde X$ を交互に生成するマルコフ連鎖を考えます。</p>

<script type="math/tex; mode=display">X_t\sim P_\theta(X\vert\tilde X_{t-1}),\,\tilde X_t\sim\mathcal C(\tilde X\vert X_t).</script>

<p>$\theta=\theta_n$ の場合を考えます。
これを $X_t$ に関するマルコフ連鎖と考えたときの遷移作用素は $\tilde X$ を積分消去して $T_n(X_t\vert X_{t-1})=\int P_{\theta_n}(X_t\vert\tilde X)\,\mathcal C(\tilde X\vert X)\,d\tilde X$ となります。
$T_n$ が定めるマルコフ連鎖が漸近分布を持つとは限りませんが、持つ場合にはそれを $\pi_n$ とおきます。
また $T=\lim_{n\to\infty}T_n$ とします。
このとき、[5]では以下の定理が成り立つことを示しています。</p>

<p><strong>定理1</strong> $P_{\theta_n}(X\vert\tilde X)$ が $\mathcal P(X\vert\tilde X)$ のconsistent estimatorで、かつ $T_n$ が定めるマルコフ連鎖がエルゴード的なら、$n\to\infty$ において $\pi_n(X)$ は $\mathcal P(X)$ に収束する。</p>

<p>つまり上のマルコフ連鎖が既約（任意の状態間を移動できる）で非周期的ならば $P_{\theta}(X\vert\tilde X)$ を正しく推定することで真のデータ生成分布が得られるということを言っています。</p>

<p>[5]が重要だと主張しているポイントは、破壊された入力 $\tilde X$ に対する $\mathcal P(X\vert\tilde X)$ を推定する方が $\mathcal P(X)$ を直接推定するより簡単だという点です。
$\mathcal P(X)$ は一般に多峰性の複雑な形になりますが、$\mathcal P(X\vert\tilde X)$ は破壊分布 $\mathcal C(\tilde X\vert X)$ が十分局所的なら、ほとんど一つのモード（峰）だけが支配的になり、推定が容易になると言っています。</p>

<h3 id="walkback">Walkbackアルゴリズム</h3>

<p>直前の段落で述べたポイントでは $\tilde X$ が $X$ に十分近ければ推定できるという話でしたが、上のマルコフ連鎖においてギブスサンプリングを回しているうちに $\tilde X_t$ がもとの $X_0$ から離れていってしまう可能性があります。
特にデータ多様体から離れた位置ではモデルがあまり学習されないため、一度多様体の近くを離れてしまうと誤ったモード(spurious modes)に吸い込まれていく可能性があります。
[5]ではこれに対処するために、複数回ギブスサンプリングを回して得られた $\tilde X_t$ についても訓練データに入れる（つまり $P_\theta(X_0\vert\tilde X_t)$ も最大化する）という方法を提案しています。
これを、遠くに行ってしまったサンプルをもとの場所に引き戻す、という意味で<strong>Walkbackアルゴリズム</strong>と呼んでいます。
サンプリングを回す回数は適当に決めます（[5]では幾何分布からサンプリングして決めています）。
WalkbackアルゴリズムはもとのDAEと同じ分布を学習することも示されていますが、もとのDAEよりも経験的に性能が良いそうです。</p>

<p>Walkbackアルゴリズムは、モデルから生成されたサンプルを訓練データに使うという意味で、RBMに対するContrastive Divergence（特にCD-$k$アルゴリズム）と似ています。</p>

<h2 id="generative-stochastic-network-gsn">Generative Stochastic Network (GSN)</h2>

<p>Yoshua Bengioは一般化DAEをさらに一般化した枠組みを提案しています[6]。
一般化DAEの枠組みでは潜在変数が含まれておらず、すべて $P_\theta(X\vert\tilde X)$ に込められていました。
このモデルに複雑な構造（例えば深層モデル）を持たせるために、一般化DAEに潜在変数を加えたものがGSNです。</p>

<p>GSNでは、一般化DAEにおけるマルコフ連鎖が潜在変数 $H_t$ を用いて次のように置き換えられます。</p>

<script type="math/tex; mode=display">H_{t+1}\sim P_{\theta_1}(H\vert H_t, X_t),\,X_{t+1}\sim P_{\theta_2}(X\vert H_{t+1}).</script>

<p>ここで $H_{t+1}$ を生成する式はノイズ変数 $Z_t$ を使って $H_{t+1}=f_{\theta_1}(X_t, Z_t, H_t)$ と表されるとします。
一般化DAEの場合にはこの $P_{\theta_1}(H\vert H_t, X_t)$ が破壊分布 $\mathcal C(\tilde X_t\vert X_t)$ になっています（つまり $\tilde X_t$ が潜在変数となります）。</p>

<p>[6]ではGSNにおいても一般化DAEと同様の収束性が成り立つことを示しています。
つまり上のマルコフ連鎖に対しても、一般化DAEと同じような仮定のもとで漸近分布がデータ生成分布 $\mathcal P(X)$ に収束します。</p>

<p>GSNではギブスサンプリングの過程で潜在変数 $H_t$ を記憶することができる点が一般化DAEとの重要な違いのようです。
論文ではDeep Boltzmann Machineのサンプリングの計算グラフと同じように計算が進むようなGSNを例として作っています（[6]Figure 3）。
これは隠れ層が3層あるモデルで、奇数目の層と偶数目の層を交互にサンプリングします。
入力層をサンプリングしたあとには一般化DAEのマルコフ連鎖同様にノイズを加えます。
また、隠れ層の各ユニットは活性化関数の前後にノイズを入れているようです（確率的ニューロンと呼んでいます[7]）。</p>

<p>Walkbackアルゴリズムにより、複数ステップ後の復元結果を学習に使うことができます。
DBMと異なりGSN全体は（確率的な）ニューラルネットなので、この復元層からの学習には誤差逆伝播法が使えます。
これにより、GSN全体は生成モデルでありながら、深いモデルでも教師あり学習の種々のテクニックが利用できるのが利点です。
例えば畳み込みレイヤーやプーリング、AdaGrad、Dropoutなどがそのまま使えます。</p>

<p>また、GSNは入力に欠損値がある場合にも使え、観測されている入力値から欠損値を補完することができます。
観測されている入力値 $X^{(s)}$ を固定した状態でギブスサンプリングを回せば $\mathcal P(X^{(-s)}\vert X^{(s)})$ からサンプリングできます。
ここで $X^{(-s)}$ は残りの入力変数です。</p>

<h2 id="section">まとめ</h2>

<p>今日はDenoising Autoencoderの定式化と様々な解釈から2段階の一般化（一般化DAEとGenerative Stochastic Network）を紹介しました。
生成モデルの解析周りは個人的に慣れない分野で、今回の話の中にも咀嚼できていない点が多々ありますが、DAEという一つのモデルからこれだけ多様な話題が出てくるのは面白いなあと思っています。
一般化について、特にGSNについては、具体的な応用が来年には出てくるのかなと想像しています。
特に教師あり学習のテクニックが使えるというのが気になっています。
これがブレイクスルーになるかはわかりませんが、これからも追っていきたいです。</p>

<p>MLACについては他の記事も読ませていただいており、今回の記事執筆自体も自分自身勉強になりました。
MLACの著者の皆様、特に企画をしてくださった@naoya_tさん、ありがとうございました。</p>

<h2 id="section-1">参考文献</h2>

<p>[1] Pascal Vincent, Hugo Larochelle, Yoshua Bengio and Pierre-Antoine Manzagol. <a href="http://www.iro.umontreal.ca/~vincentp/Publications/vincent_icml_2008.pdf">Extracting and Composing Robust Features with Denoising Autoencoders</a>. <em>Proc. of ICML, 2008</em>.<br />
[2] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot and Yoshua Bengio. <a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf">Contractive Auto-Encoders: Explicit Invariance During Feature Extraction</a>. <em>Proc. of ICML, 2011</em>.<br />
[3] Pascal Vincent. <a href="http://www.iro.umontreal.ca/~vincentp/Publications/DenoisingScoreMatching_NeuralComp2011.pdf">A Connection Between Score Matching and Denoising Autoencoders</a>. <em>Neural Computation, 2011</em>.<br />
[4] Aapo Hyv̈arinen. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/Hyvarinen05.pdf">Estimation of Non-Normalized Statistical Models by Score Matching</a>. <em>Journal of Machine Learning Research 6, 2005</em>.<br />
[5] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. <a href="http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models">Generalized Denoising Auto-Encoders as Generative Models</a>. <em>NIPS, 2013</em>.<br />
[6] Yoshua Bengio. <a href="http://arxiv.org/abs/1306.1091">Deep Generative Stochastic Networks Trainable by Backprop</a>. <em>arXiv:1306.1091, 2013</em>.<br />
[7] Yoshua Bengio. <a href="http://arxiv.org/abs/1305.2982">Estimating or Propagating Gradients Through Stochastic Neurons</a>. <em>arXiv:1305.2982, 2013</em>.</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>といっても僕自身これらを自分で実装して動かしたわけではなく、論文追っかけてるだけなので誰かの受け売り状態なのですが。。。<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>この定義は統計学におけるもともとの「スコア関数』の定義とは異なるそうです。もともとは確率モデルの対数密度勾配を、入力変数ではなくモデルのパラメータについて取ったものがスコア関数でした。<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
